## train reranker with fixed (default) backbone and global mining
#python3 -u train_reranker.py  --backbone deit --aggregation=gem --mining global --neg_hardness 100   --resume logs/global_retrieval/best_model.pth --save_dir rerank --lr 0.0005 --cos --fc_output_dim 256 --num_workers 8 --warmup 5 --optim adamw --epochs_num 50 --patience 25 --negs_num_per_query 1 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 4 --infer_batch_size 96 --rerank_batch_size 2 --save_best 0
#
## finetune global retrieval and reranking modules together, use --fix 0 to enable training of global retrieval module, add --finetune 1 may have better result
#python3 -u train_reranker.py --fix 0  --backbone deit --aggregation=gem --mining partial  --save_dir finetune --resume logs/rerank_2023-07-29_13-20-29/best_model.pth --lr 0.00001 --cos --fc_output_dim 256 --num_workers 8 --warmup 5 --optim adamw --epochs_num 20 --patience 50 --negs_num_per_query 2 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 2 --infer_batch_size 96 --rerank_batch_size 2 --save_best 0
#
## finetune on Pitts30k for urban datasets
## python3 -u train_reranker.py --fix 0 --dataset_name=pitts30k --backbone deit --aggregation=gem --mining full --datasets_folder ../datasets_vg/datasets  --save_dir pitts30k_finetune --resume CVPR23_DeitS_Rerank.pth  --lr 0.00001 --fc_output_dim 256  --cos --warmup 5 --optim adamw --epochs_num 50 --patience 10 --negs_num_per_query 1 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 16 --infer_batch_size 256 --rerank_batch_size 4

# use backbone="resnet50" to train reranker with resnet backbone
# train reranker with fixed (default) backbone and global mining

#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 2  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 1000 --negs_num_per_query 15 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 4 --infer_batch_size 96 --rerank_batch_size 2 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"
python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 1200 --num_local_nomask 1000 --negs_num_per_query 15 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 4 --infer_batch_size 96 --rerank_batch_size 2 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"  --seed 42
#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 1000 --negs_num_per_query 15 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 4 --infer_batch_size 96 --rerank_batch_size 2 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"  --seed 46
#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 1000 --negs_num_per_query 15 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 4 --infer_batch_size 96 --rerank_batch_size 2 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"   --seed 104


#aggregation='gem', aggregation_select='gem', attention_depth=1, backbone='deit', brightness=None, cache_refresh_rate=1000, channel_bottleneck=128, contrast=None, cos=False, criterion='triplet', cross_limit=30, dataset_name='Pittsburgh30k', datasets_folder='/media/flztiii/7320bc0c-a136-44a4-86f2-0ffc2c3b3a9a/llg_vpr_dataset', device='cuda', dist_method='l2', efficient_ram_testing=False, epochs_num=100, fc_output_dim=256, finetune=0, fix=1, freeze=0, freeze_te=None, horizontal_flip=False, hue=None, hypercolumn=0, infer_batch_size=96, l2='before_pool', learn_adj=True, local_dim=128, lr=0.0004, lr_crn_layer=0.005, lr_crn_net=0.0005, majority_weight=0.01, margin=0.1, mining='global', neg_hardness=100, neg_samples_num=1000, negs_num_per_query=15, netvlad_clusters=64, non_local=False, num_classes=2, num_graph_layers=3, num_local=1000, num_non_local=1, num_pairs=5, num_workers=8, off_the_shelf='imagenet', optim='adamw', patience=5, pca_dataset_folder=None, pca_dim=None, pretrain='imagenet', queries_per_epoch=5000, rand_perspective=None, random_resized_crop=None, random_rotation=None, recall_values=[1, 5, 10, 20, 100], reg_top=5, rerank_batch_size=2, rerank_loss='ce', rerank_model='GCNRerank', resize=[480, 640], resume='logs/global_retrieval/best_model.pth', runpath='runs', saturation=None, save_best=0, save_dir='logs/GCNrerank_2023-08-31_20-46-20', schedule=[10, 20], seed=42, temperature=0.5, test=False, test_method='hard_resize', train_batch_size=4, train_positives_dist_threshold=10, trunc_te=None, val_positive_dist_threshold=25, warmup=0)

#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 1000 --negs_num_per_query 30 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 2 --infer_batch_size 96 --rerank_batch_size 2 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"
#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.5  --epochs_num 100 --patience 5 --num_local 600 --negs_num_per_query 10 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 8 --infer_batch_size 96 --rerank_batch_size 6 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"


#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.6  --epochs_num 100 --patience 4 --negs_num_per_query 10 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 10 --infer_batch_size 96 --rerank_batch_size 8 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"
#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.4  --epochs_num 100 --patience 4 --negs_num_per_query 10 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 10 --infer_batch_size 96 --rerank_batch_size 8 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"
#python3 -u train_reranker.py  --backbone deit  --aggregation=gem --mining global --neg_hardness 100  --warmup 0  --resume logs/global_retrieval/best_model.pth  --save_dir GCNrerank --lr 0.0004 --fc_output_dim 256 --num_workers 8  --optim adamw --temperature 0.1  --epochs_num 100 --patience 4 --negs_num_per_query 10 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 10 --infer_batch_size 96 --rerank_batch_size 8 --rerank_model GCNRerank --save_best 0 --rerank_loss "ce"

# finetune global retrieval and reranking modules together, use --fix 0 to enable training of global retrieval module, add --finetune 1 may have better result
#python3 -u train_reranker.py --fix 0  --backbone deit --aggregation=gem --mining global  --save_dir finetune_GCNRerank --resume logs/GCNrerank_2023-08-24_15-34-28/best_model.pth --lr 0.000005  --fc_output_dim 256 --num_workers 8 --warmup 0 --optim adamw --epochs_num 200 --patience 5 --negs_num_per_query 10 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 1 --infer_batch_size 96 --rerank_batch_size 8 --save_best 0 --rerank_model GCNRerank

# finetune on Pitts30k for urban datasets
# python3 -u train_reranker.py --fix 0 --dataset_name=pitts30k --backbone deit --aggregation=gem --mining full --datasets_folder ../datasets_vg/datasets  --save_dir pitts30k_finetune --resume CVPR23_DeitS_Rerank.pth  --lr 0.00001 --fc_output_dim 256  --cos --warmup 5 --optim adamw --epochs_num 50 --patience 10 --negs_num_per_query 1 --queries_per_epoch 5000 --cache_refresh_rate 1000 --train_batch_size 16 --infer_batch_size 256 --rerank_batch_size 4

# use backbone="resnet50" to train reranker with resnet backbone